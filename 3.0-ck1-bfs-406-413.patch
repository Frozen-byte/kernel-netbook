Improve the preempt call by explicitly setting a task that will be used on
lookup in earliest_deadline_task to avoid iterating over all tasks. Set the
effective runqueue priority for further calls to try_preempt based on the
preempting task's priority. Thus on the small chance that something else
tries to preempt the runqueue before the new tasks gets the CPU, it will
compare to the previously successful preempting task. This should avoid a
preempting task from rescheduling a runqueue and then the CPU deciding to
take a different task instead of the preempting one.

Clean up a number of variables from unnecessary longs to ints and ints to
bools.

Microoptimise around try preempt by settings highest_prio to that of the
task that's trying to preempt to avoid unnecessary comparisons.

Break sole affinity on cpu offline -after- the cpu has been set to be offline.

-ck

---
 include/linux/sched.h |   32 +++----
 kernel/sched_bfs.c    |  218 +++++++++++++++++++++++++++-----------------------
 2 files changed, 135 insertions(+), 115 deletions(-)

Index: linux-3.0-ck1/include/linux/sched.h
===================================================================
--- linux-3.0-ck1.orig/include/linux/sched.h	2011-10-18 12:10:48.000000000 +1100
+++ linux-3.0-ck1/include/linux/sched.h	2011-10-18 12:15:09.993136165 +1100
@@ -1233,10 +1233,12 @@
 	struct task_struct *wake_entry;
 #endif
 #if defined(CONFIG_SMP) || defined(CONFIG_SCHED_BFS)
-	int on_cpu;
+	bool on_cpu;
 #endif
 #endif
-	int on_rq;
+#ifndef CONFIG_SCHED_BFS
+	bool on_rq;
+#endif
 
 	int prio, static_prio, normal_prio;
 	unsigned int rt_priority;
@@ -1247,7 +1249,7 @@
 	u64 last_ran;
 	u64 sched_time; /* sched_clock time spent running */
 #ifdef CONFIG_SMP
-	int sticky; /* Soft affined flag */
+	bool sticky; /* Soft affined flag */
 #endif
 	unsigned long rt_timeout;
 #else /* CONFIG_SCHED_BFS */
@@ -1596,7 +1598,7 @@
 };
 
 #ifdef CONFIG_SCHED_BFS
-extern int grunqueue_is_locked(void);
+extern bool grunqueue_is_locked(void);
 extern void grq_unlock_wait(void);
 extern void cpu_scaling(int cpu);
 extern void cpu_nonscaling(int cpu);
@@ -1611,15 +1613,15 @@
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO"BFS CPU scheduler v0.406 by Con Kolivas.\n");
+	printk(KERN_INFO"BFS CPU scheduler v0.413 by Con Kolivas.\n");
 }
 
-static inline int iso_task(struct task_struct *p)
+static inline bool iso_task(struct task_struct *p)
 {
 	return (p->policy == SCHED_ISO);
 }
-extern void remove_cpu(unsigned long cpu);
-extern int above_background_load(void);
+extern void remove_cpu(int cpu);
+extern bool above_background_load(void);
 #else /* CFS */
 extern int runqueue_is_locked(int cpu);
 static inline void cpu_scaling(int cpu)
@@ -1642,12 +1644,12 @@
 	printk(KERN_INFO"CFS CPU scheduler.\n");
 }
 
-static inline int iso_task(struct task_struct *p)
+static inline bool iso_task(struct task_struct *p)
 {
-	return 0;
+	return false;
 }
 
-static inline void remove_cpu(unsigned long cpu)
+static inline void remove_cpu(int cpu)
 {
 }
 
@@ -2665,21 +2667,21 @@
  */
 #ifdef CONFIG_SMP
 
-static inline unsigned int task_cpu(const struct task_struct *p)
+static inline int task_cpu(const struct task_struct *p)
 {
 	return task_thread_info(p)->cpu;
 }
 
-extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
+extern void set_task_cpu(struct task_struct *p, int cpu);
 
 #else
 
-static inline unsigned int task_cpu(const struct task_struct *p)
+static inline int task_cpu(const struct task_struct *p)
 {
 	return 0;
 }
 
-static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
+static inline void set_task_cpu(struct task_struct *p, int cpu)
 {
 }
 
Index: linux-3.0-ck1/kernel/sched_bfs.c
===================================================================
--- linux-3.0-ck1.orig/kernel/sched_bfs.c	2011-10-18 12:10:48.136460211 +1100
+++ linux-3.0-ck1/kernel/sched_bfs.c	2011-10-18 12:15:42.123137305 +1100
@@ -168,7 +168,7 @@
 #ifdef CONFIG_SMP
 	unsigned long qnr; /* queued not running */
 	cpumask_t cpu_idle_map;
-	int idle_cpus;
+	bool idle_cpus;
 #endif
 	int noc; /* num_online_cpus stored and updated when it changes */
 	u64 niffies; /* Nanosecond jiffies */
@@ -227,7 +227,7 @@
 #endif
 #endif
 
-	struct task_struct *curr, *idle, *stop;
+	struct task_struct *curr, *idle, *stop, *preempt;
 	struct mm_struct *prev_mm;
 
 	/* Stored data about rq->curr to work outside grq lock */
@@ -236,7 +236,7 @@
 	int rq_time_slice;
 	u64 rq_last_ran;
 	int rq_prio;
-	int rq_running; /* There is a task running */
+	bool rq_running; /* There is a task running */
 
 	/* Accurate timekeeping data */
 	u64 timekeep_clock;
@@ -247,20 +247,20 @@
 
 #ifdef CONFIG_SMP
 	int cpu;		/* cpu of this runqueue */
-	int online;
-	int scaling; /* This CPU is managed by a scaling CPU freq governor */
+	bool online;
+	bool scaling; /* This CPU is managed by a scaling CPU freq governor */
 	struct task_struct *sticky_task;
 
 	struct root_domain *rd;
 	struct sched_domain *sd;
-	unsigned long *cpu_locality; /* CPU relative cache distance */
+	int *cpu_locality; /* CPU relative cache distance */
 #ifdef CONFIG_SCHED_SMT
-	int (*siblings_idle)(unsigned long cpu);
+	bool (*siblings_idle)(int cpu);
 	/* See if all smt siblings are idle */
 	cpumask_t smt_siblings;
 #endif
 #ifdef CONFIG_SCHED_MC
-	int (*cache_idle)(unsigned long cpu);
+	bool (*cache_idle)(int cpu);
 	/* See if all cache siblings are idle */
 	cpumask_t cache_siblings;
 #endif
@@ -271,7 +271,7 @@
 #endif
 	u64 clock, old_clock, last_tick;
 	u64 clock_task;
-	int dither;
+	bool dither;
 
 #ifdef CONFIG_SCHEDSTATS
 
@@ -439,7 +439,7 @@
 	update_rq_clock_task(rq, delta);
 }
 
-static inline int task_running(struct task_struct *p)
+static inline bool task_running(struct task_struct *p)
 {
 	return p->on_cpu;
 }
@@ -537,7 +537,7 @@
  * This interface allows printk to be called with the runqueue lock
  * held and know whether or not it is OK to wake up the klogd.
  */
-inline int grunqueue_is_locked(void)
+inline bool grunqueue_is_locked(void)
 {
 	return raw_spin_is_locked(&grq.lock);
 }
@@ -574,7 +574,7 @@
  * this lockless for overhead reasons since the occasional wrong result
  * is harmless.
  */
-int above_background_load(void)
+bool above_background_load(void)
 {
 	struct task_struct *cpu_curr;
 	unsigned long cpu;
@@ -630,12 +630,12 @@
 }
 #endif /* __ARCH_WANT_UNLOCKED_CTXSW */
 
-static inline int deadline_before(u64 deadline, u64 time)
+static inline bool deadline_before(u64 deadline, u64 time)
 {
 	return (deadline < time);
 }
 
-static inline int deadline_after(u64 deadline, u64 time)
+static inline bool deadline_after(u64 deadline, u64 time)
 {
 	return (deadline > time);
 }
@@ -646,7 +646,7 @@
  * A task that is currently running will have ->on_cpu set but not on the
  * grq run list.
  */
-static inline int task_queued(struct task_struct *p)
+static inline bool task_queued(struct task_struct *p)
 {
 	return (!list_empty(&p->run_list));
 }
@@ -665,7 +665,7 @@
  * To determine if it's safe for a task of SCHED_IDLEPRIO to actually run as
  * an idle task, we ensure none of the following conditions are met.
  */
-static int idleprio_suitable(struct task_struct *p)
+static bool idleprio_suitable(struct task_struct *p)
 {
 	return (!freezing(p) && !signal_pending(p) &&
 		!(task_contributes_to_load(p)) && !(p->flags & (PF_EXITING)));
@@ -675,7 +675,7 @@
  * To determine if a task of SCHED_ISO can run in pseudo-realtime, we check
  * that the iso_refractory flag is not set.
  */
-static int isoprio_suitable(void)
+static bool isoprio_suitable(void)
 {
 	return !grq.iso_refractory;
 }
@@ -730,6 +730,18 @@
 	return (rr_interval * task_prio_ratio(p) / 128);
 }
 
+static void resched_task(struct task_struct *p);
+
+static inline void preempt_rq(struct rq *rq, struct task_struct *p)
+{
+	rq->preempt = p;
+	/* We set the runqueue's apparent priority to the task that will
+	 * replace the current one in case something else tries to preempt
+	 * this runqueue before p gets scheduled */
+	rq->rq_prio = p->prio;
+	resched_task(rq->curr);
+}
+
 #ifdef CONFIG_SMP
 /*
  * qnr is the "queued but not running" count which is the total number of
@@ -757,30 +769,28 @@
  * It's cheaper to maintain a binary yes/no if there are any idle CPUs on the
  * idle_cpus variable than to do a full bitmask check when we are busy.
  */
-static inline void set_cpuidle_map(unsigned long cpu)
+static inline void set_cpuidle_map(int cpu)
 {
 	if (likely(cpu_online(cpu))) {
 		cpu_set(cpu, grq.cpu_idle_map);
-		grq.idle_cpus = 1;
+		grq.idle_cpus = true;
 	}
 }
 
-static inline void clear_cpuidle_map(unsigned long cpu)
+static inline void clear_cpuidle_map(int cpu)
 {
 	cpu_clear(cpu, grq.cpu_idle_map);
 	if (cpus_empty(grq.cpu_idle_map))
-		grq.idle_cpus = 0;
+		grq.idle_cpus = false;
 }
 
-static int suitable_idle_cpus(struct task_struct *p)
+static bool suitable_idle_cpus(struct task_struct *p)
 {
 	if (!grq.idle_cpus)
-		return 0;
+		return false;
 	return (cpus_intersects(p->cpus_allowed, grq.cpu_idle_map));
 }
 
-static void resched_task(struct task_struct *p);
-
 #define CPUIDLE_DIFF_THREAD	(1)
 #define CPUIDLE_DIFF_CORE	(2)
 #define CPUIDLE_CACHE_BUSY	(4)
@@ -805,14 +815,16 @@
  * Other node, other CPU, busy threads.
  */
 static void
-resched_best_mask(unsigned long best_cpu, struct rq *rq, cpumask_t *tmpmask)
+resched_best_mask(cpumask_t *tmpmask, struct task_struct *p)
 {
-	unsigned long cpu_tmp, best_ranking;
-
-	best_ranking = ~0UL;
+	unsigned int best_ranking = CPUIDLE_DIFF_NODE | CPUIDLE_THREAD_BUSY |
+		CPUIDLE_DIFF_CPU | CPUIDLE_CACHE_BUSY | CPUIDLE_DIFF_CORE |
+		CPUIDLE_DIFF_THREAD;
+	int cpu_tmp, best_cpu = task_cpu(p);
+	struct rq *rq = task_rq(p);
 
 	for_each_cpu_mask(cpu_tmp, *tmpmask) {
-		unsigned long ranking;
+		unsigned int ranking;
 		struct rq *tmp_rq;
 
 		ranking = 0;
@@ -845,7 +857,7 @@
 		}
 	}
 
-	resched_task(cpu_rq(best_cpu)->curr);
+	preempt_rq(cpu_rq(best_cpu), p);
 }
 
 static void resched_best_idle(struct task_struct *p)
@@ -853,7 +865,7 @@
 	cpumask_t tmpmask;
 
 	cpus_and(tmpmask, p->cpus_allowed, grq.cpu_idle_map);
-	resched_best_mask(task_cpu(p), task_rq(p), &tmpmask);
+	resched_best_mask(&tmpmask, p);
 }
 
 static inline void resched_suitable_idle(struct task_struct *p)
@@ -868,15 +880,15 @@
  */
 void cpu_scaling(int cpu)
 {
-	cpu_rq(cpu)->scaling = 1;
+	cpu_rq(cpu)->scaling = true;
 }
 
 void cpu_nonscaling(int cpu)
 {
-	cpu_rq(cpu)->scaling = 0;
+	cpu_rq(cpu)->scaling = false;
 }
 
-static inline int scaling_rq(struct rq *rq)
+static inline bool scaling_rq(struct rq *rq)
 {
 	return rq->scaling;
 }
@@ -894,15 +906,15 @@
 	return grq.nr_running;
 }
 
-static inline void set_cpuidle_map(unsigned long cpu)
+static inline void set_cpuidle_map(int cpu)
 {
 }
 
-static inline void clear_cpuidle_map(unsigned long cpu)
+static inline void clear_cpuidle_map(int cpu)
 {
 }
 
-static inline int suitable_idle_cpus(struct task_struct *p)
+static inline bool suitable_idle_cpus(struct task_struct *p)
 {
 	return uprq->curr == uprq->idle;
 }
@@ -923,9 +935,9 @@
  * Although CPUs can scale in UP, there is nowhere else for tasks to go so this
  * always returns 0.
  */
-static inline int scaling_rq(struct rq *rq)
+static inline bool scaling_rq(struct rq *rq)
 {
-	return 0;
+	return false;
 }
 #endif /* CONFIG_SMP */
 EXPORT_SYMBOL_GPL(cpu_scaling);
@@ -1009,7 +1021,7 @@
 }
 
 #ifdef CONFIG_SMP
-void set_task_cpu(struct task_struct *p, unsigned int cpu)
+void set_task_cpu(struct task_struct *p, int cpu)
 {
 #ifdef CONFIG_LOCKDEP
 	/*
@@ -1032,17 +1044,17 @@
 
 static inline void clear_sticky(struct task_struct *p)
 {
-	p->sticky = 0;
+	p->sticky = false;
 }
 
-static inline int task_sticky(struct task_struct *p)
+static inline bool task_sticky(struct task_struct *p)
 {
 	return p->sticky;
 }
 
 /* Reschedule the best idle CPU that is not this one. */
 static void
-resched_closest_idle(struct rq *rq, unsigned long cpu, struct task_struct *p)
+resched_closest_idle(struct rq *rq, int cpu, struct task_struct *p)
 {
 	cpumask_t tmpmask;
 
@@ -1050,7 +1062,7 @@
 	cpu_clear(cpu, tmpmask);
 	if (cpus_empty(tmpmask))
 		return;
-	resched_best_mask(cpu, rq, &tmpmask);
+	resched_best_mask(&tmpmask, p);
 }
 
 /*
@@ -1061,7 +1073,7 @@
  * latency at all times.
  */
 static inline void
-swap_sticky(struct rq *rq, unsigned long cpu, struct task_struct *p)
+swap_sticky(struct rq *rq, int cpu, struct task_struct *p)
 {
 	if (rq->sticky_task) {
 		if (rq->sticky_task == p) {
@@ -1092,13 +1104,13 @@
 {
 }
 
-static inline int task_sticky(struct task_struct *p)
+static inline bool task_sticky(struct task_struct *p)
 {
-	return 0;
+	return false;
 }
 
 static inline void
-swap_sticky(struct rq *rq, unsigned long cpu, struct task_struct *p)
+swap_sticky(struct rq *rq, int cpu, struct task_struct *p)
 {
 }
 
@@ -1111,9 +1123,9 @@
  * Move a task off the global queue and take it to a cpu for it will
  * become the running task.
  */
-static inline void take_task(struct rq *rq, struct task_struct *p)
+static inline void take_task(int cpu, struct task_struct *p)
 {
-	set_task_cpu(p, cpu_of(rq));
+	set_task_cpu(p, cpu);
 	dequeue_task(p);
 	clear_sticky(p);
 	dec_qnr();
@@ -1209,7 +1221,7 @@
 unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 {
 	unsigned long flags;
-	int running, on_rq;
+	bool running, on_rq;
 	unsigned long ncsw;
 	struct rq *rq;
 
@@ -1339,19 +1351,20 @@
  * between themselves, they cooperatively multitask. An idle rq scores as
  * prio PRIO_LIMIT so it is always preempted.
  */
-static inline int
+static inline bool
 can_preempt(struct task_struct *p, int prio, u64 deadline)
 {
 	/* Better static priority RT task or better policy preemption */
 	if (p->prio < prio)
-		return 1;
+		return true;
 	if (p->prio > prio)
-		return 0;
+		return false;
 	/* SCHED_NORMAL, BATCH and ISO will preempt based on deadline */
 	if (!deadline_before(p->deadline, deadline))
-		return 0;
-	return 1;
+		return false;
+	return true;
 }
+
 #ifdef CONFIG_SMP
 #ifdef CONFIG_HOTPLUG_CPU
 /*
@@ -1359,15 +1372,15 @@
  * still wants runtime. This happens to kernel threads during suspend/halt and
  * disabling of CPUs.
  */
-static inline int online_cpus(struct task_struct *p)
+static inline bool online_cpus(struct task_struct *p)
 {
 	return (likely(cpus_intersects(cpu_online_map, p->cpus_allowed)));
 }
 #else /* CONFIG_HOTPLUG_CPU */
 /* All available CPUs are always online without hotplug. */
-static inline int online_cpus(struct task_struct *p)
+static inline bool online_cpus(struct task_struct *p)
 {
-	return 1;
+	return true;
 }
 #endif
 
@@ -1375,11 +1388,11 @@
  * Check to see if p can run on cpu, and if not, whether there are any online
  * CPUs it can run on instead.
  */
-static inline int needs_other_cpu(struct task_struct *p, int cpu)
+static inline bool needs_other_cpu(struct task_struct *p, int cpu)
 {
 	if (unlikely(!cpu_isset(cpu, p->cpus_allowed)))
-		return 1;
-	return 0;
+		return true;
+	return false;
 }
 
 /*
@@ -1389,9 +1402,8 @@
 static void try_preempt(struct task_struct *p, struct rq *this_rq)
 {
 	struct rq *highest_prio_rq = this_rq;
+	int cpu, highest_prio;
 	u64 latest_deadline;
-	unsigned long cpu;
-	int highest_prio;
 	cpumask_t tmp;
 
 	/*
@@ -1416,7 +1428,7 @@
 		return;
 
 	latest_deadline = 0;
-	highest_prio = -1;
+	highest_prio = p->prio;
 
 	for_each_cpu_mask(cpu, tmp) {
 		struct rq *rq;
@@ -1441,9 +1453,9 @@
 	resched_task(highest_prio_rq->curr);
 }
 #else /* CONFIG_SMP */
-static inline int needs_other_cpu(struct task_struct *p, int cpu)
+static inline bool needs_other_cpu(struct task_struct *p, int cpu)
 {
-	return 0;
+	return false;
 }
 
 static void try_preempt(struct task_struct *p, struct rq *this_rq)
@@ -1536,12 +1548,13 @@
  * Returns %true if @p was woken up, %false if it was already running
  * or @state didn't match @p's state.
  */
-static int try_to_wake_up(struct task_struct *p, unsigned int state,
+static bool try_to_wake_up(struct task_struct *p, unsigned int state,
 			  int wake_flags)
 {
+	bool success = false;
 	unsigned long flags;
-	int cpu, success = 0;
 	struct rq *rq;
+	int cpu;
 
 	get_cpu();
 
@@ -1688,7 +1701,7 @@
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
 #endif
 
-	p->on_cpu = 0;
+	p->on_cpu = false;
 	clear_sticky(p);
 
 #ifdef CONFIG_PREEMPT
@@ -2703,7 +2716,7 @@
 	}
 }
 
-static int rq_running_iso(struct rq *rq)
+static bool rq_running_iso(struct rq *rq)
 {
 	return rq->rq_prio == ISO_PRIO;
 }
@@ -2921,14 +2934,21 @@
  * selected by the earliest deadline.
  */
 static inline struct
-task_struct *earliest_deadline_task(struct rq *rq, struct task_struct *idle)
+task_struct *earliest_deadline_task(struct rq *rq, int cpu, struct task_struct *idle)
 {
-	u64 dl, earliest_deadline = 0; /* Initialise to silence compiler */
-	struct task_struct *p, *edt = idle;
-	unsigned int cpu = cpu_of(rq);
+	struct task_struct *p, *edt, *rqpreempt = rq->preempt;
+	u64 dl, uninitialized_var(earliest_deadline);
 	struct list_head *queue;
 	int idx = 0;
 
+	if (rqpreempt) {
+		rq->preempt = NULL;
+		if (task_queued(rqpreempt)) {
+			edt = rqpreempt;
+			goto out_take;
+		}
+	}
+	edt = idle;
 retry:
 	idx = find_next_bit(grq.prio_bitmap, PRIO_LIMIT, idx);
 	if (idx >= PRIO_LIMIT)
@@ -2970,8 +2990,8 @@
 
 		/*
 		 * No rt tasks. Find the earliest deadline task. Now we're in
-		 * O(n) territory. This is what we silenced the compiler for:
-		 * edt will always start as idle.
+		 * O(n) territory. This is what we silenced the compiler for
+		 * with uninitialized_var(): edt will always start as idle.
 		 */
 		if (edt == idle ||
 		    deadline_before(dl, earliest_deadline)) {
@@ -2985,7 +3005,7 @@
 		goto out;
 	}
 out_take:
-	take_task(rq, edt);
+	take_task(cpu, edt);
 out:
 	return edt;
 }
@@ -3042,9 +3062,9 @@
 	rq->rq_policy = p->policy;
 	rq->rq_prio = p->prio;
 	if (p != rq->idle)
-		rq->rq_running = 1;
+		rq->rq_running = true;
 	else
-		rq->rq_running = 0;
+		rq->rq_running = false;
 }
 
 static void reset_rq_task(struct rq *rq, struct task_struct *p)
@@ -3079,9 +3099,9 @@
 	update_clocks(rq);
 	update_cpu_clock(rq, prev, 0);
 	if (rq->clock - rq->last_tick > HALF_JIFFY_NS)
-		rq->dither = 0;
+		rq->dither = false;
 	else
-		rq->dither = 1;
+		rq->dither = true;
 
 	clear_tsk_need_resched(prev);
 
@@ -3150,7 +3170,7 @@
 		schedstat_inc(rq, sched_goidle);
 		set_cpuidle_map(cpu);
 	} else {
-		next = earliest_deadline_task(rq, idle);
+		next = earliest_deadline_task(rq, cpu, idle);
 		if (likely(next->prio != PRIO_LIMIT)) {
 			prefetch(next);
 			prefetch_stack(next);
@@ -3168,8 +3188,8 @@
 			unstick_task(rq, prev);
 		set_rq_task(rq, next);
 		grq.nr_switches++;
-		prev->on_cpu = 0;
-		next->on_cpu = 1;
+		prev->on_cpu = false;
+		next->on_cpu = true;
 		rq->curr = next;
 		++*switch_count;
 
@@ -4476,7 +4496,7 @@
 	return 0;
 }
 
-static inline int should_resched(void)
+static inline bool should_resched(void)
 {
 	return need_resched() && !(preempt_count() & PREEMPT_ACTIVE);
 }
@@ -5064,8 +5084,6 @@
 	/* cpu has to be offline */
 	BUG_ON(cpu_online(this_cpu));
 
-	break_sole_affinity(this_cpu, idle);
-
 	__setscheduler(idle, rq, SCHED_FIFO, STOP_PRIO);
 
 	activate_idle_task(idle);
@@ -5287,7 +5305,7 @@
 {
 	if (!rq->online) {
 		cpumask_set_cpu(cpu_of(rq), rq->rd->online);
-		rq->online = 1;
+		rq->online = true;
 	}
 }
 
@@ -5295,7 +5313,7 @@
 {
 	if (rq->online) {
 		cpumask_clear_cpu(cpu_of(rq), rq->rd->online);
-		rq->online = 0;
+		rq->online = false;
 	}
 }
 
@@ -5350,6 +5368,7 @@
 			BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
 			set_rq_offline(rq);
 		}
+		break_sole_affinity(cpu, idle);
 		grq.noc = num_online_cpus();
 		grq_unlock_irqrestore(&flags);
 		break;
@@ -6682,14 +6701,14 @@
  * Cheaper version of the below functions in case support for SMT and MC is
  * compiled in but CPUs have no siblings.
  */
-static int sole_cpu_idle(unsigned long cpu)
+static bool sole_cpu_idle(int cpu)
 {
 	return rq_idle(cpu_rq(cpu));
 }
 #endif
 #ifdef CONFIG_SCHED_SMT
 /* All this CPU's SMT siblings are idle */
-static int siblings_cpu_idle(unsigned long cpu)
+static bool siblings_cpu_idle(int cpu)
 {
 	return cpumask_subset(&(cpu_rq(cpu)->smt_siblings),
 			      &grq.cpu_idle_map);
@@ -6697,7 +6716,7 @@
 #endif
 #ifdef CONFIG_SCHED_MC
 /* All this CPU's shared cache siblings are idle */
-static int cache_cpu_idle(unsigned long cpu)
+static bool cache_cpu_idle(int cpu)
 {
 	return cpumask_subset(&(cpu_rq(cpu)->cache_siblings),
 			      &grq.cpu_idle_map);
@@ -6755,8 +6774,7 @@
 	for_each_online_cpu(cpu) {
 		struct rq *rq = cpu_rq(cpu);
 		for_each_domain(cpu, sd) {
-			unsigned long locality;
-			int other_cpu;
+			int locality, other_cpu;
 
 #ifdef CONFIG_SCHED_SMT
 			if (sd->level == SD_LV_SIBLING) {
@@ -6842,13 +6860,14 @@
 		rq = cpu_rq(i);
 		rq->user_pc = rq->nice_pc = rq->softirq_pc = rq->system_pc =
 			      rq->iowait_pc = rq->idle_pc = 0;
-		rq->dither = 0;
+		rq->dither = false;
+		rq->preempt = NULL;
 #ifdef CONFIG_SMP
 		rq->sticky_task = NULL;
 		rq->last_niffy = 0;
 		rq->sd = NULL;
 		rq->rd = NULL;
-		rq->online = 0;
+		rq->online = false;
 		rq->cpu = i;
 		rq_attach_root(rq, &def_root_domain);
 #endif
@@ -6877,8 +6896,7 @@
 		rq->cache_idle = sole_cpu_idle;
 		cpumask_set_cpu(i, &rq->cache_siblings);
 #endif
-		rq->cpu_locality = kmalloc(nr_cpu_ids * sizeof(unsigned long),
-					   GFP_NOWAIT);
+		rq->cpu_locality = kmalloc(nr_cpu_ids * sizeof(int), GFP_ATOMIC);
 		for_each_possible_cpu(j) {
 			if (i == j)
 				rq->cpu_locality[j] = 0;
